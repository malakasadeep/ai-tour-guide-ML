{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üá±üá∞ Production-Grade LLM Fine-Tuning: Sri Lankan AI Tour Guide\n",
    "\n",
    "**Model:** Meta-Llama-3.1-8B-Instruct (4-bit Quantized)  \n",
    "**Framework:** Unsloth + LoRA (Low-Rank Adaptation)  \n",
    "**Environment:** Google Colab Free Tier (T4 GPU)  \n",
    "**Use Case:** Fine-tuning for culturally-aware Sri Lankan tourism assistance\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Workflow Overview\n",
    "\n",
    "1. **Environment Setup** - Install dependencies and verify GPU\n",
    "2. **Model Loading** - Load quantized Llama 3.1 8B with LoRA adapters\n",
    "3. **Data Preparation** - Format JSONL dataset for instruction tuning\n",
    "4. **Training** - Fine-tune with optimized hyperparameters\n",
    "5. **Inference Testing** - Compare before/after performance\n",
    "6. **Model Export** - Save as GGUF for local deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1"
   },
   "source": [
    "## Step 1: Environment Setup & Dependency Installation\n",
    "\n",
    "Installing the Unsloth optimization framework along with required libraries:\n",
    "- **unsloth**: 2-5x faster training, 80% less memory usage\n",
    "- **xformers**: Memory-efficient attention mechanisms\n",
    "- **trl**: Transformer Reinforcement Learning (SFTTrainer)\n",
    "- **peft**: Parameter-Efficient Fine-Tuning (LoRA)\n",
    "- **accelerate**: Distributed training utilities\n",
    "- **bitsandbytes**: 4-bit quantization support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth and dependencies (suppress output for cleaner notebook)\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_check"
   },
   "source": [
    "### Verify GPU Availability\n",
    "\n",
    "Ensuring we have access to a CUDA-capable GPU (T4 expected on Colab Free Tier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU availability and specifications\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n",
    "    print(f\"‚úÖ GPU Detected: {gpu_name}\")\n",
    "    print(f\"üìä Total GPU Memory: {gpu_memory:.2f} GB\")\n",
    "    print(f\"üî¢ CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected. Please enable GPU in Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
    "    raise RuntimeError(\"GPU is required for this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2"
   },
   "source": [
    "## Step 2: Model Loading with Unsloth Optimization\n",
    "\n",
    "Loading the **Meta-Llama-3.1-8B-Instruct** model in 4-bit quantization mode with LoRA adapters.\n",
    "\n",
    "### LoRA Configuration:\n",
    "- **r=16**: Rank of the low-rank matrices (higher = more capacity but slower)\n",
    "- **lora_alpha=16**: Scaling factor for LoRA updates\n",
    "- **Target Modules**: All linear projection layers in the transformer architecture\n",
    "  - `q_proj`, `k_proj`, `v_proj`: Query, Key, Value projections (attention)\n",
    "  - `o_proj`: Output projection (attention)\n",
    "  - `gate_proj`, `up_proj`, `down_proj`: Feed-forward network layers\n",
    "\n",
    "This configuration maximizes model expressiveness while remaining memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 2048  # Maximum sequence length for training\n",
    "LOAD_IN_4BIT = True    # Use 4-bit quantization\n",
    "\n",
    "# Load the model and tokenizer with Unsloth optimizations\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect optimal dtype (float16 for T4)\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {MODEL_NAME}\")\n",
    "print(f\"üìè Max Sequence Length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"üîß Quantization: 4-bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lora_config"
   },
   "source": [
    "### Configure LoRA Adapters\n",
    "\n",
    "Applying LoRA (Low-Rank Adaptation) to enable efficient fine-tuning. Instead of updating all 8 billion parameters, we only train small adapter matrices, reducing memory usage by ~80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apply_lora"
   },
   "outputs": [],
   "source": [
    "# Apply LoRA adapters to the model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank - higher values = more parameters but better performance\n",
    "    lora_alpha=16,  # LoRA scaling factor (typically equal to r)\n",
    "    lora_dropout=0,  # Dropout for LoRA layers (0 = no dropout, recommended for small datasets)\n",
    "    target_modules=[\n",
    "        \"q_proj\",    # Query projection (attention)\n",
    "        \"k_proj\",    # Key projection (attention)\n",
    "        \"v_proj\",    # Value projection (attention)\n",
    "        \"o_proj\",    # Output projection (attention)\n",
    "        \"gate_proj\", # Gate projection (FFN)\n",
    "        \"up_proj\",   # Up projection (FFN)\n",
    "        \"down_proj\", # Down projection (FFN)\n",
    "    ],\n",
    "    bias=\"none\",  # Don't add LoRA to bias terms\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Enable gradient checkpointing for memory efficiency\n",
    "    random_state=42,  # Seed for reproducibility\n",
    "    use_rslora=False,  # Rank-stabilized LoRA (optional, can improve stability)\n",
    "    loftq_config=None,  # LoftQ quantization config (advanced)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters applied successfully\")\n",
    "print(f\"üìä Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"üìä Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"üìà Trainable %: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3"
   },
   "source": [
    "## Step 3: Data Preparation & Formatting\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "Our JSONL dataset contains instruction/response pairs. We'll format them into the **Alpaca instruction format** with a custom system prompt that defines the AI's personality and behavior.\n",
    "\n",
    "### System Prompt Design\n",
    "\n",
    "The system prompt establishes:\n",
    "- **Identity**: Expert Sri Lankan Tour Guide\n",
    "- **Tone**: Warm, hospitable (using \"Ayubowan\" greeting)\n",
    "- **Capabilities**: Accurate travel advice, logistical planning\n",
    "- **Technical Requirements**: Strict JSON formatting for tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "system_prompt"
   },
   "outputs": [],
   "source": [
    "# Define the custom system prompt for the Sri Lankan Tour Guide\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert Sri Lankan Tour Guide. You speak with a warm, hospitable tone ('Ayubowan'). You provide accurate travel advice, check for logistical constraints, and format tool calls strictly as JSON.\"\"\"\n",
    "\n",
    "# Alpaca-style instruction template\n",
    "ALPACA_PROMPT = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "print(\"‚úÖ Prompt templates defined\")\n",
    "print(f\"\\nüìù System Prompt:\\n{SYSTEM_PROMPT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_data"
   },
   "source": [
    "### Upload Dataset\n",
    "\n",
    "Upload your `finetune_dataset.jsonl` file using the file upload widget below.\n",
    "\n",
    "**Note**: If running locally or in a different environment, modify the file path accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_file"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Upload the dataset file\n",
    "print(\"üì§ Please upload your finetune_dataset.jsonl file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify the file was uploaded\n",
    "if 'finetune_dataset.jsonl' in uploaded:\n",
    "    print(\"‚úÖ Dataset uploaded successfully\")\n",
    "    DATASET_PATH = \"finetune_dataset.jsonl\"\n",
    "else:\n",
    "    print(\"‚ùå Expected file 'finetune_dataset.jsonl' not found\")\n",
    "    print(f\"Available files: {list(uploaded.keys())}\")\n",
    "    # Use the first uploaded file if available\n",
    "    if uploaded:\n",
    "        DATASET_PATH = list(uploaded.keys())[0]\n",
    "        print(f\"‚ö†Ô∏è Using {DATASET_PATH} instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_format_data"
   },
   "source": [
    "### Load and Format Dataset\n",
    "\n",
    "Loading the JSONL file and converting each record into the Alpaca instruction format. The system prompt is integrated into the instruction field to condition the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "format_dataset"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file into a list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "def format_alpaca_prompt(instruction, input_text, output_text):\n",
    "    \"\"\"Format a single example into Alpaca instruction format.\"\"\"\n",
    "    return ALPACA_PROMPT.format(instruction, input_text, output_text)\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format dataset examples for instruction tuning.\n",
    "    \n",
    "    This function:\n",
    "    1. Combines the system prompt with the task instruction\n",
    "    2. Uses the input field as context\n",
    "    3. Formats everything into the Alpaca template\n",
    "    \"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    \n",
    "    texts = []\n",
    "    for instruction, input_text, output_text in zip(instructions, inputs, outputs):\n",
    "        # Combine system prompt with the specific instruction\n",
    "        full_instruction = f\"{SYSTEM_PROMPT}\\n\\n{instruction}\"\n",
    "        \n",
    "        # Format using Alpaca template\n",
    "        text = format_alpaca_prompt(full_instruction, input_text, output_text)\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Load the JSONL dataset\n",
    "print(f\"üìÇ Loading dataset from: {DATASET_PATH}\")\n",
    "raw_data = load_jsonl(DATASET_PATH)\n",
    "print(f\"‚úÖ Loaded {len(raw_data)} examples\")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "print(f\"‚úÖ Dataset converted to Hugging Face format\")\n",
    "print(f\"üìä Dataset features: {dataset.features}\")\n",
    "print(f\"üìä Dataset size: {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "debug_data"
   },
   "source": [
    "### Debug: Inspect Formatted Examples\n",
    "\n",
    "Before training, let's verify the data formatting is correct by examining a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "print_example"
   },
   "outputs": [],
   "source": [
    "# Format the dataset and inspect the first example\n",
    "formatted_dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    num_proc=2,  # Use 2 processes for faster formatting\n",
    "    remove_columns=dataset.column_names,  # Remove original columns, keep only 'text'\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã FORMATTED EXAMPLE (First Training Sample)\")\n",
    "print(\"=\" * 80)\n",
    "print(formatted_dataset[0]['text'])\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úÖ Total formatted examples: {len(formatted_dataset)}\")\n",
    "print(f\"üìè Average text length: {sum(len(x['text']) for x in formatted_dataset) / len(formatted_dataset):.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4"
   },
   "source": [
    "## Step 4: Training Configuration & Fine-Tuning\n",
    "\n",
    "### Hyperparameter Rationale\n",
    "\n",
    "- **max_seq_length=2048**: Supports longer conversations while fitting in T4 GPU memory\n",
    "- **batch_size=2 + grad_accum=4**: Effective batch size of 8 (balances speed vs. memory)\n",
    "- **learning_rate=2e-4**: Standard for LoRA fine-tuning (higher than full fine-tuning)\n",
    "- **adamw_8bit**: Memory-efficient optimizer (8-bit quantized Adam)\n",
    "- **max_steps**: Configurable training duration (100 steps ‚âà quick prototype)\n",
    "\n",
    "### Training Strategy\n",
    "\n",
    "Using **Supervised Fine-Tuning (SFT)** via the TRL library's `SFTTrainer`, optimized for instruction-following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURABLE HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "MAX_STEPS = 100  # üîß CHANGE THIS VALUE to train longer/shorter\n",
    "                  # Typical values: 100-200 (quick test), 500-1000 (good results), 2000+ (production)\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output & Logging\n",
    "    output_dir=\"./outputs\",\n",
    "    run_name=\"sri-lankan-tour-guide-llama-3.1-8b\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Training Duration\n",
    "    max_steps=MAX_STEPS,\n",
    "    \n",
    "    # Batch Size & Gradient Accumulation\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 2 * 4 = 8\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"adamw_8bit\",  # 8-bit Adam optimizer for memory efficiency\n",
    "    weight_decay=0.01,   # L2 regularization\n",
    "    warmup_steps=10,     # Gradual learning rate warmup\n",
    "    \n",
    "    # Precision & Performance\n",
    "    fp16=not torch.cuda.is_bf16_supported(),  # Use FP16 if BF16 not available\n",
    "    bf16=torch.cuda.is_bf16_supported(),      # Use BF16 if supported (better for training)\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,  # Keep only last 2 checkpoints to save disk space\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for cleaner output\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"\\nüìä Training Configuration:\")\n",
    "print(f\"   Max Steps: {MAX_STEPS}\")\n",
    "print(f\"   Batch Size (per device): {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning Rate: {training_args.learning_rate}\")\n",
    "print(f\"   Optimizer: {training_args.optim}\")\n",
    "print(f\"   Precision: {'BF16' if training_args.bf16 else 'FP16'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "initialize_trainer"
   },
   "source": [
    "### Initialize SFT Trainer\n",
    "\n",
    "Creating the trainer with our model, dataset, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_trainer"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",  # Column containing the formatted prompts\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=2,  # Number of processes for data loading\n",
    "    packing=False,  # Don't pack multiple examples into one sequence (cleaner for instruction tuning)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SFTTrainer initialized successfully\")\n",
    "print(f\"üìä Training dataset size: {len(formatted_dataset)} examples\")\n",
    "print(f\"üìä Estimated training time: ~{MAX_STEPS * 2 / 60:.1f} minutes (approximate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5_inference_before"
   },
   "source": [
    "## Step 5a: Inference Testing - BEFORE Fine-Tuning\n",
    "\n",
    "Testing the base model's performance on our target query to establish a baseline. This allows us to compare the improvements after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_before"
   },
   "outputs": [],
   "source": [
    "# Test query\n",
    "TEST_QUERY = \"I want to visit Sigiriya but I am on a budget. Any advice?\"\n",
    "\n",
    "# Prepare the prompt in Alpaca format\n",
    "test_instruction = f\"{SYSTEM_PROMPT}\\n\\nYou are Travion, a friendly and knowledgeable Sri Lankan tour guide AI assistant. Help tourists plan their trips, provide cultural insights, and share local knowledge with warmth and authenticity.\"\n",
    "test_prompt = ALPACA_PROMPT.format(test_instruction, TEST_QUERY, \"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ BASELINE INFERENCE - BEFORE FINE-TUNING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Query: {TEST_QUERY}\\n\")\n",
    "\n",
    "# Set model to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "# Decode and display\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "# Extract only the response part (after \"### Response:\")\n",
    "if \"### Response:\" in generated_text:\n",
    "    response = generated_text.split(\"### Response:\")[1].strip()\n",
    "else:\n",
    "    response = generated_text\n",
    "\n",
    "print(f\"Response (BEFORE):\\n{response}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start_training"
   },
   "source": [
    "## Step 5b: Start Fine-Tuning\n",
    "\n",
    "Beginning the training process. This cell will display real-time loss metrics and progress.\n",
    "\n",
    "**Expected behavior:**\n",
    "- Loss should decrease over time\n",
    "- Training will take approximately 3-10 minutes for 100 steps on T4 GPU\n",
    "- Checkpoints will be saved every 50 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"üöÄ Starting fine-tuning...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_duration = end_time - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"‚è±Ô∏è  Total training time: {training_duration / 60:.2f} minutes\")\n",
    "print(f\"üìä Final training loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"üìà Steps completed: {trainer_stats.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5_inference_after"
   },
   "source": [
    "## Step 5c: Inference Testing - AFTER Fine-Tuning\n",
    "\n",
    "Testing the fine-tuned model on the same query to evaluate improvements. We expect:\n",
    "- More culturally appropriate responses (\"Ayubowan\" greetings)\n",
    "- Better knowledge of Sri Lankan tourism\n",
    "- More helpful budget-specific advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_after"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üß™ INFERENCE - AFTER FINE-TUNING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Query: {TEST_QUERY}\\n\")\n",
    "\n",
    "# Set model to inference mode (IMPORTANT: re-apply for optimized inference)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Tokenize and generate with the same prompt\n",
    "inputs = tokenizer([test_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "# Decode and display\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "if \"### Response:\" in generated_text:\n",
    "    response = generated_text.split(\"### Response:\")[1].strip()\n",
    "else:\n",
    "    response = generated_text\n",
    "\n",
    "print(f\"Response (AFTER):\\n{response}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüí° Compare the before/after responses to evaluate fine-tuning effectiveness!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6"
   },
   "source": [
    "## Step 6: Model Export & Saving\n",
    "\n",
    "### Export Options\n",
    "\n",
    "We'll save the fine-tuned model in multiple formats:\n",
    "\n",
    "1. **GGUF (q4_k_m)** - For local deployment with Ollama/llama.cpp\n",
    "2. **Hugging Face Format** - For deployment with transformers library\n",
    "3. **LoRA Adapters Only** - Lightweight format (just the fine-tuned weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_gguf"
   },
   "source": [
    "### 6.1: Save as GGUF (Quantized for Ollama)\n",
    "\n",
    "GGUF format enables running the model locally with tools like Ollama. The `q4_k_m` quantization provides a good balance between size and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_gguf"
   },
   "outputs": [],
   "source": [
    "# Save the model in GGUF format with q4_k_m quantization\n",
    "print(\"üíæ Saving model in GGUF format (q4_k_m quantization)...\")\n",
    "print(\"‚ö†Ô∏è  This may take 5-10 minutes...\\n\")\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    \"sri_lankan_tour_guide_gguf\",  # Output directory\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",  # Quantization: q4_k_m (good balance of size/quality)\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ GGUF model saved to: ./sri_lankan_tour_guide_gguf/\")\n",
    "print(\"\\nüì¶ To use with Ollama:\")\n",
    "print(\"   1. Download the .gguf file from the output directory\")\n",
    "print(\"   2. Create a Modelfile:\")\n",
    "print(\"      FROM ./model.gguf\")\n",
    "print(f\"      SYSTEM '{SYSTEM_PROMPT}'\")\n",
    "print(\"   3. Run: ollama create sri-lankan-guide -f Modelfile\")\n",
    "print(\"   4. Run: ollama run sri-lankan-guide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_hf"
   },
   "source": [
    "### 6.2: Save in Hugging Face Format\n",
    "\n",
    "This format is compatible with the `transformers` library for deployment in Python applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_hf"
   },
   "outputs": [],
   "source": [
    "# Save the full model in Hugging Face format (16-bit)\n",
    "print(\"üíæ Saving model in Hugging Face format (16-bit)...\\n\")\n",
    "\n",
    "model.save_pretrained(\n",
    "    \"sri_lankan_tour_guide_hf\",\n",
    "    tokenizer=tokenizer,\n",
    "    save_method=\"merged_16bit\",  # Merge LoRA weights and save in 16-bit\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Hugging Face model saved to: ./sri_lankan_tour_guide_hf/\")\n",
    "print(\"\\nüì¶ To use this model:\")\n",
    "print(\"   from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
    "print(\"   model = AutoModelForCausalLM.from_pretrained('./sri_lankan_tour_guide_hf')\")\n",
    "print(\"   tokenizer = AutoTokenizer.from_pretrained('./sri_lankan_tour_guide_hf')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_lora"
   },
   "source": [
    "### 6.3: Save LoRA Adapters Only (Lightweight)\n",
    "\n",
    "If you want to save only the fine-tuned weights (very small, ~50-100 MB), use this option. You'll need to load them on top of the base model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export_lora"
   },
   "outputs": [],
   "source": [
    "# Save only the LoRA adapter weights (lightweight)\n",
    "print(\"üíæ Saving LoRA adapters only...\\n\")\n",
    "\n",
    "model.save_pretrained(\"sri_lankan_tour_guide_lora\")\n",
    "tokenizer.save_pretrained(\"sri_lankan_tour_guide_lora\")\n",
    "\n",
    "print(\"‚úÖ LoRA adapters saved to: ./sri_lankan_tour_guide_lora/\")\n",
    "print(\"\\nüì¶ To use LoRA adapters:\")\n",
    "print(\"   from unsloth import FastLanguageModel\")\n",
    "print(\"   model, tokenizer = FastLanguageModel.from_pretrained(\")\n",
    "print(\"       model_name='unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit',\")\n",
    "print(\"       adapter_dir='./sri_lankan_tour_guide_lora'\")\n",
    "print(\"   )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_files"
   },
   "source": [
    "### 6.4: Download Files to Local Machine\n",
    "\n",
    "Zip and download the exported models for local use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip_download"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# Create zip files for easy download\n",
    "print(\"üì¶ Creating zip archives...\\n\")\n",
    "\n",
    "# Zip GGUF model\n",
    "if os.path.exists(\"sri_lankan_tour_guide_gguf\"):\n",
    "    shutil.make_archive(\"sri_lankan_tour_guide_gguf\", 'zip', \"sri_lankan_tour_guide_gguf\")\n",
    "    print(\"‚úÖ Created: sri_lankan_tour_guide_gguf.zip\")\n",
    "\n",
    "# Zip LoRA adapters\n",
    "if os.path.exists(\"sri_lankan_tour_guide_lora\"):\n",
    "    shutil.make_archive(\"sri_lankan_tour_guide_lora\", 'zip', \"sri_lankan_tour_guide_lora\")\n",
    "    print(\"‚úÖ Created: sri_lankan_tour_guide_lora.zip\")\n",
    "\n",
    "print(\"\\nüì• Download the files below:\")\n",
    "print(\"   - For Ollama: Download sri_lankan_tour_guide_gguf.zip\")\n",
    "print(\"   - For lightweight deployment: Download sri_lankan_tour_guide_lora.zip\")\n",
    "print(\"\\nüí° Tip: The GGUF file is recommended for local deployment with Ollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "---\n",
    "\n",
    "## üéâ Training Complete!\n",
    "\n",
    "### Summary\n",
    "\n",
    "You've successfully fine-tuned a Llama 3.1 8B model for Sri Lankan tourism assistance. The model now:\n",
    "\n",
    "‚úÖ Responds with culturally appropriate greetings (\"Ayubowan\")  \n",
    "‚úÖ Provides accurate Sri Lankan travel advice  \n",
    "‚úÖ Maintains a warm, hospitable tone  \n",
    "‚úÖ Handles budget-conscious queries effectively  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Evaluate Performance**: Test the model with diverse queries to ensure quality\n",
    "2. **Deploy Locally**: Use the GGUF file with Ollama for local testing\n",
    "3. **Iterate**: If needed, adjust hyperparameters and retrain:\n",
    "   - Increase `MAX_STEPS` for better convergence (try 500-1000)\n",
    "   - Adjust `learning_rate` if loss plateaus\n",
    "   - Expand your dataset with more diverse examples\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "- **Monitoring**: Track model performance with real user queries\n",
    "- **Safety**: Implement content filtering for inappropriate outputs\n",
    "- **Versioning**: Tag your models with version numbers and training metadata\n",
    "- **A/B Testing**: Compare fine-tuned vs. base model in production\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check:\n",
    "- Unsloth documentation: https://github.com/unslothai/unsloth\n",
    "- Hugging Face TRL: https://github.com/huggingface/trl\n",
    "- Ollama documentation: https://ollama.ai/docs\n",
    "\n",
    "Happy fine-tuning! üöÄüá±üá∞"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
